{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel \n",
    "import torch\n",
    "import numpy as np\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sentence_transformers import util\n",
    "\n",
    "# sbert_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-cls-token\")\n",
    "# sbert_model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-cls-token\")\n",
    "\n",
    "sbert_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "sbert_model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASHA:\n",
    "def sbert_phrase_sim(phrase1, phrase2, encoded_input, sbert_tokenizer, sbert_model):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    phrase1: str phrase from the first amr\n",
    "    phrase2: str phrase from the second amr\n",
    "    encoded_input: transformers.tokenization_utils_base.BatchEncoding; both sentences([sent1, sent2]) encoded by a tokenizer. \n",
    "    Returns:\n",
    "    cosine_score: int; cosine similarity between two phrases based on the sentence embeddings\n",
    "    \"\"\"\n",
    "    #encode both phrases to compare with the sentences\n",
    "    encoded_phrase1 = sbert_tokenizer(phrase1) \n",
    "    encoded_phrase2 = sbert_tokenizer(phrase2)\n",
    "    #first phrase\n",
    "    first_sent = encoded_input[\"input_ids\"][0] #get the ids from the sentence\n",
    "    first_phrase = encoded_phrase1[\"input_ids\"][1:-1] #get the ids for the wanted words\n",
    "    #find the index in the sentence for words in the phrase\n",
    "    sent_index = [torch.eq(x, first_sent) for x in torch.tensor(first_phrase)]\n",
    "   \n",
    "    if len(sent_index) == 1: \n",
    "        condition1 = sent_index[0] #eg. condition = [False, False, False, True, True, False]\n",
    "    elif len(sent_index) == 2:\n",
    "        condition1 = sent_index[0] + sent_index[1]\n",
    "    elif len(sent_index) == 3:\n",
    "        condition1 = sent_index[0] + sent_index[1] + sent_index[2]\n",
    "    \n",
    "    attention = encoded_input[\"attention_mask\"][0] #e.g [1,1,1,1,1] \n",
    "    print(condition1)\n",
    "    new_attention1 = attention.where(condition1, torch.tensor(0)) #only the wanted words eg.[0,0,1,1,0]\n",
    "    \n",
    "    \n",
    "    #do the same for the second phrase\n",
    "    second_sent = encoded_ids[\"input_ids\"][1]\n",
    "    second_phrase  = encoded_phrase2[\"input_ids\"][1:-1]\n",
    "    sent2_index = [torch.eq(x, second_sent) for x in torch.tensor(second_phrase)]\n",
    "    \n",
    "    if len(sent2_index) == 1: \n",
    "        condition2 = sent2_index[0]\n",
    "    elif len(sent2_index) == 2:\n",
    "        condition2 = sent2_index[0] + sent2_index[1]\n",
    "    elif len(sent2_index) == 3:\n",
    "        condition2 = sent2_index[0] + sent2_index[1] + sent2_index[2]    \n",
    "\n",
    "    attention2 = encoded_ids[\"attention_mask\"][1]\n",
    "    new_attention2 = attention.where(condition2, torch.tensor(0))\n",
    "    \n",
    "    #get the new attention for the mean pooling\n",
    "    new_attention = torch.stack([new_attention1, new_attention2])\n",
    "    print(new_attention)\n",
    "    with torch.no_grad():\n",
    "        model_output = sbert_model(**encoded_input)\n",
    "        \n",
    "    sentence_embeddings = mean_pooling(model_output,new_attention)\n",
    "    #sentence_embeddings2 = mean_pooling(model_output,encoded_ids[\"attention_mask\"]) test sim for the full sents\n",
    "    \n",
    "    cos = torch.nn.CosineSimilarity(0)\n",
    "    \n",
    "    cosine = cos(sentence_embeddings[0], sentence_embeddings[1])\n",
    "    cosine_score = float(cosine.squeeze().detach().numpy())\n",
    "   \n",
    "    \n",
    "    return cosine_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DENIS:\n",
    "\n",
    "def x_in_y(query, base):\n",
    "    try:\n",
    "        l = len(query)\n",
    "    except TypeError:\n",
    "        l = 1\n",
    "        query = type(base)((query,))\n",
    "\n",
    "    for i in range(len(base)):\n",
    "        if base[i:i+l] == query:\n",
    "            return list(range(i, i+l))\n",
    "    return False\n",
    "\n",
    "def get_new_attention_mask(phrase1, phrase2, encoded_input, sbert_tokenizer):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    phrase1: str phrase from the first amr\n",
    "    phrase2: str phrase from the second amr\n",
    "    encoded_input: transformers.tokenization_utils_base.BatchEncoding; both sentences([sent1, sent2]) encoded by a tokenizer. \n",
    "    Returns:\n",
    "    cosine_score: int; cosine similarity between two phrases based on the sentence embeddings\n",
    "    \"\"\"\n",
    "    # encode both phrases to compare with the sentences\n",
    "    encoded_phrase1 = sbert_tokenizer(phrase1) \n",
    "    encoded_phrase2 = sbert_tokenizer(phrase2)\n",
    "    # first phrase\n",
    "    sents = encoded_input['input_ids']\n",
    "    sent1 = encoded_input['input_ids'][0] # get the ids from the sentence\n",
    "    sent2 = encoded_input['input_ids'][1]\n",
    "    phrase1 = encoded_phrase1['input_ids'][1:-1] # get the ids for the wanted words\n",
    "    phrase2 = encoded_phrase2['input_ids'][1:-1]\n",
    "    # find the index in the sentence for words in the phrase    \n",
    "    indices_matched1 = x_in_y(phrase1, sent1.tolist())\n",
    "    indices_matched2 = x_in_y(phrase2, sent2.tolist())\n",
    "    \n",
    "    if not (indices_matched1 and indices_matched2):\n",
    "        return 0\n",
    "    \n",
    "    new_attention = torch.zeros_like(sents, dtype=torch.bool)\n",
    "    new_attention[0, torch.tensor(indices_matched1)] = True\n",
    "    new_attention[1, torch.tensor(indices_matched2)] = True\n",
    "    return new_attention\n",
    "    \n",
    "\n",
    "def sbert_sim(model_output, phrase1, phrase2):\n",
    "    sentence_embeddings = mean_pooling(model_output, \n",
    "                                       get_new_attention_mask(phrase1, phrase2, encoded_ids, sbert_tokenizer))\n",
    "    \n",
    "    return cos(sentence_embeddings[0], sentence_embeddings[1]).item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6100732684135437\n"
     ]
    }
   ],
   "source": [
    "# sentences = [\"Chinese lunar rover lands on moon\",\"China lands robot rover on moon\"]\n",
    "# phrase1 = \"Chinese lunar rover\"\n",
    "# phrase2 = \"robot\"\n",
    "\n",
    "# sentences = [\"a young cat sprints\", \"a kitten runs\"]\n",
    "# phrase1 = \"young cat\"\n",
    "# phrase2 = \"kitten\"\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(0)\n",
    "sentences = [\"we eat french fries\", \"we eat chips\"]\n",
    "phrase1 = \"french fries\"\n",
    "phrase2 = \"chips\"\n",
    "\n",
    "encoded_ids = sbert_tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "        \n",
    "print(sbert_sim(sbert_model(**encoded_ids), phrase1, phrase2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "#Sentences we want sentence embeddings for\n",
    "sentences = ['dog walks', 'cat', 'kitten runs']\n",
    "\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(sentence_embeddings.shape)\n",
    "print(encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  3899,  7365,   102],\n",
       "        [  101,  4937,   102,     0],\n",
       "        [  101, 18401,  3216,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 0],\n",
       "        [1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6995,  0.2457,  1.4855, -0.4822, -0.4247],\n",
       "        [ 0.6995,  0.2457,  1.4855, -0.4822, -0.4247]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((sentence_embeddings[0, :5], \n",
    "            sentence_embeddings[0, :5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_model(**encoded_ids)[0].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
