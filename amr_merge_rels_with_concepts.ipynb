{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate relationships with the parent node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import penman\n",
    "from penman import layout\n",
    "from penman.graph import Graph\n",
    "from penman.transform import reify_attributes\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def pprint(l, reified=False, **args):\n",
    "    if isinstance(l, dict):\n",
    "        print('Key\\tValue')\n",
    "        for k, v in l.items():            \n",
    "            print(f'{k}\\t{v}', **args)\n",
    "            \n",
    "    elif isinstance(l, list) or isinstance(l, tuple) or isinstance(l, set):\n",
    "        for el in l:\n",
    "            print(el, **args)\n",
    "            \n",
    "    elif isinstance(l, penman.Graph):\n",
    "        if reified:\n",
    "            l = penman.encode(l)\n",
    "            l = reify_rename_graph_from_string(l)\n",
    "        print(penman.encode(l), **args)\n",
    "        \n",
    "    elif isinstance(l, penman.Tree):\n",
    "        if reified:\n",
    "            l = penman.format(l)\n",
    "            l = reify_rename_graph_from_string(l)\n",
    "            print(penman.encode(l), **args)\n",
    "        else:\n",
    "            print(penman.format(l), **args)\n",
    "        \n",
    "    elif isinstance(l, str):\n",
    "        if reified:\n",
    "            l = reify_rename_graph_from_string(l)\n",
    "            print(penman.encode(l), **args)\n",
    "        else:\n",
    "            print(penman.format(penman.parse(l)), **args)\n",
    "            \n",
    "    else:\n",
    "        raise ValueError('Unknown type')\n",
    "    print(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': |0, possible-01, (2, 3)|,\n",
       " '0.0': |0.0, do-02, (4, 5)|,\n",
       " '0.0.0': |0.0.0, improve-01, (6, 7)|,\n",
       " '0.0.0.0': |0.0.0.0, score, (9, 10)|,\n",
       " '0.0.0.0.0': |0.0.0.0.0, credit, (8, 9)|,\n",
       " '0.0.0.0.1': |0.0.0.0.1, i, (3, 4)|,\n",
       " '0.0.1': |0.0.1, amr-unknown, (0, 1)|,\n",
       " '0.0.1.0': |0.0.1.0, more, (1, 2)|,\n",
       " '_ROOT_': |_ROOT_, _ROOT_, (-1, -1)|}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### This is an alternative way (maybe even a better one, because it uses the original code) \n",
    "### to map nodes to structures/concepts and alignments such as '0.0.0.0': |0.0.0.0, score, (9, 10)|\n",
    "\n",
    "from AMR2Text.toolkit.tamr_aligner.amr.aligned import Alignment\n",
    "\n",
    "amr2text_alingnment_path = Path('.')/'AMR2Text'/'processed'/'corpus_a.mrp.new_aligned.txt'\n",
    "with open(amr2text_alingnment_path) as f:\n",
    "    amrs = f.read().strip().split('\\n\\n')\n",
    "    amrs = [amr.split('\\n') for amr in amrs]\n",
    "\n",
    "amr_id3 = amrs[3]\n",
    "al = Alignment(amr_id3)\n",
    "al.alignments.alignments\n",
    "al.nodes_by_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMRAnalysis:\n",
    "    def __init__(self, amr2text_alingnment_path, keep_meta=False, concat_rel=True):\n",
    "        self.amr2text_alingnment_path = amr2text_alingnment_path        \n",
    "        self.keep_meta = keep_meta\n",
    "        self.info_dict = {}\n",
    "        if concat_rel:\n",
    "            self.concat_rel()\n",
    "    \n",
    "    @staticmethod\n",
    "    def reify_rename_graph_from_string(amr_string):\n",
    "    \n",
    "        g1 = reify_attributes(penman.decode(amr_string))\n",
    "        t1 = layout.configure(g1)\n",
    "        t1.reset_variables(fmt='MRPNode-{i}')\n",
    "        g1 = layout.interpret(t1)\n",
    "\n",
    "        return g1\n",
    "    \n",
    "    @staticmethod\n",
    "    def alignment_labels2mrp_labels(amr_string):\n",
    "        \"\"\"Currently works only on reified graphs\"\"\"\n",
    "\n",
    "        amr_graph = AMRAnalysis.reify_rename_graph_from_string(amr_string)\n",
    "        epidata, triples = amr_graph.epidata, amr_graph.triples\n",
    "        cur_label, popped = '0', False\n",
    "        labels_dict = {cur_label:amr_graph.top}\n",
    "        for triple in triples:        \n",
    "            cur_node = triple[0]        \n",
    "            epi = epidata[triple]\n",
    "            if epi and isinstance(epi[0], penman.layout.Push):\n",
    "                cur_node = epi[0].variable\n",
    "                if not popped:\n",
    "                    cur_label += '.0'\n",
    "                labels_dict[cur_label] = cur_node\n",
    "                popped = False            \n",
    "            elif epi and isinstance(epi[0], penman.layout.Pop):\n",
    "                pops_count = epi.count(epi[0])\n",
    "                split = cur_label.split('.')\n",
    "                if popped: \n",
    "                    split = split[:len(split)-pops_count] \n",
    "                else:\n",
    "                    split = split[:len(split)-pops_count+1]\n",
    "                split[-1] = str(int(split[-1])+1)\n",
    "                cur_label = '.'.join(split)\n",
    "                popped = True\n",
    "\n",
    "        return labels_dict, amr_graph\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_alignments_dict_from_string(alignments_string, alignment_pattern, toks, labels_dict):\n",
    "        \"\"\"\n",
    "        Somehow the alingnments string in 'new_alinged' does not contain\n",
    "        all aligned nodes that are specified below ¯\\_(ツ)_/¯ \n",
    "        \"\"\"\n",
    "        matches = re.match(alignment_pattern, alignments_string)\n",
    "        if not matches:\n",
    "            raise ValueError(f'Alignments string \"{alignments_string}\" has wrong format!\\nCould not find alignments.')\n",
    "        alignments = matches.group(1).split()\n",
    "        alignments_dict = {}\n",
    "\n",
    "        for alignment in alignments:\n",
    "            parts = alignment.split('|')\n",
    "            token_span = parts[0]\n",
    "            #indices = span.split('-')\n",
    "            #token_span = ' '.join(toks[int(indices[0]):int(indices[1])])\n",
    "            nodes = parts[1].split('+')\n",
    "            nodes = [labels_dict[node] for node in nodes]\n",
    "            for node in nodes:\n",
    "                alignments_dict[node] = token_span\n",
    "        return alignments_dict\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_alignments_dict(nodes_block, labels_dict):\n",
    "        \"\"\"\n",
    "        This function deals with the problem that was found while using the \n",
    "        function above\n",
    "        \"\"\"\n",
    "        nodes_block = [spl_line for spl_line in nodes_block if len(spl_line) == 3]\n",
    "        alignments_dict = {}\n",
    "        for spl_line in nodes_block:\n",
    "            node = spl_line[0]\n",
    "            node = labels_dict[node] # '0.0.0' --> 'MRPNode2'\n",
    "            token_span = spl_line[2]\n",
    "            alignments_dict[node] = token_span\n",
    "            \n",
    "        return alignments_dict\n",
    "\n",
    "    def extract_info(self):    \n",
    "        with open(self.amr2text_alingnment_path) as f:\n",
    "            amrs = f.read().strip().split('\\n\\n')\n",
    "            amrs = [amr.split('\\n') for amr in amrs]\n",
    "\n",
    "        alignment_pattern = re.compile(r'# ::alignments\\s(.+?)\\s::')\n",
    "        for amr_analysis in amrs:\n",
    "            amr_id = amr_analysis[0].split()[-1]\n",
    "\n",
    "            toks = amr_analysis[2].split()[2:] # first 2 tokens are: '# ::tok'\n",
    "            toks = [tok.lower() for tok in toks]\n",
    "\n",
    "            amr_string = amr_analysis[-1]\n",
    "            labels_dict, amr_graph = AMRAnalysis.alignment_labels2mrp_labels(amr_string)\n",
    "\n",
    "            alignments_string = amr_analysis[3]\n",
    "            nodes_block = [line.split()[2:] for line in amr_analysis if line.startswith('# ::node')] # first 2 tokens are: '# ::node'\n",
    "            try:\n",
    "                # function below works well, but the alignments string doesn't contain all alignments, so a new function\n",
    "                # has to be defined\n",
    "                #alignments_dict = AMRAnalysis.get_alignments_dict_from_string(alignments_string, alignment_pattern, toks, labels_dict)\n",
    "                alignments_dict = AMRAnalysis.get_alignments_dict(nodes_block, labels_dict)\n",
    "                alignments_dict = defaultdict(lambda: None, alignments_dict)\n",
    "            except KeyError as e:\n",
    "                print(amr_id)\n",
    "                pprint(amr_string, reified=True)\n",
    "                pprint(labels_dict)\n",
    "                raise e\n",
    "\n",
    "            self.info_dict[amr_id] = {'amr_string':penman.encode(amr_graph), \\\n",
    "                                      'toks':toks, \\\n",
    "                                      'alignments_dict':alignments_dict, \\\n",
    "                                      'labels_dict':labels_dict, \\\n",
    "                                      'amr_graph':amr_graph}\n",
    "            if self.keep_meta:\n",
    "                meta = amr_analysis[:3] # save '# ::id', '# ::snt' fields\n",
    "                meta = '\\n'.join(meta)\n",
    "                self.info_dict[amr_id]['meta'] =  meta\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_below(labels_dict):\n",
    "        \"\"\"\n",
    "        Finds nodes below a certain node using a dictionary of a following form\n",
    "        (located in 'info_dict[amr_id]['labels_dict']'):\n",
    "        \n",
    "        Key Value\n",
    "        0 MRPNode-0\n",
    "        0.0 MRPNode-1\n",
    "        0.0.0 MRPNode-2\n",
    "        0.0.0.0\tMRPNode-3\n",
    "        0.0.0.0.0 MRPNode-4\n",
    "        0.0.0.0.1 MRPNode-5\n",
    "        0.0.1 MRPNode-6\n",
    "        0.0.1.0 MRPNode-7\n",
    "        \n",
    "        Returns a dict where the key is the node label (e.g 'MRPNode-2') and\n",
    "        the value is a list with all nodes represented as strings below it.\n",
    "        \"\"\"\n",
    "        nodes_below_dict = defaultdict(list)\n",
    "        for key, value in labels_dict.items():\n",
    "            for k, v in labels_dict.items():\n",
    "                if k.startswith(key) and len(k) > len(key):\n",
    "                    nodes_below_dict[value].append(v)\n",
    "        return nodes_below_dict\n",
    "    \n",
    "    @staticmethod\n",
    "    def full_span(subtree_token_spans):\n",
    "        \"\"\"\n",
    "        Takes a list of token spans of a whole subtree of form:\n",
    "        and checks, if there are gaps. \n",
    "        \n",
    "        Returns a list of indices if a token span is full, else False.\n",
    "        \"\"\"\n",
    "        toks_indices = set()\n",
    "        for token_span in subtree_token_spans:\n",
    "            spl = token_span.split('-')\n",
    "            i1, i2 = int(spl[0]), int(spl[1])\n",
    "            indices = set(range(i1, i2))            \n",
    "            toks_indices.update(indices)            \n",
    "        minimum, maximum = min(toks_indices), max(toks_indices)\n",
    "        toks_indices = sorted(list(toks_indices))\n",
    "        if toks_indices == list(range(minimum, maximum+1)):\n",
    "            return toks_indices\n",
    "        return False\n",
    "    \n",
    "    def concat_rel(self, rel=':mod'): \n",
    "        if not self.info_dict:\n",
    "            self.extract_info()\n",
    "        self.graphs_concat_rel = {}\n",
    "        \n",
    "        # ONLY FOR DEBUGGING CERTAIN IDS!!!\n",
    "        # DELETE FOR NORMAL USE!!!\n",
    "        #self.info_dict = {k:v for k, v in self.info_dict.items() if k == '3'}\n",
    "        \n",
    "        for amr_id in self.info_dict:\n",
    "            triples_filtered = []\n",
    "            g = self.info_dict[amr_id]['amr_graph']\n",
    "            toks = self.info_dict[amr_id]['toks']\n",
    "            alignments_dict = self.info_dict[amr_id]['alignments_dict']\n",
    "            nodes_below_dict = AMRAnalysis.find_below(self.info_dict[amr_id]['labels_dict'])\n",
    "            instances_dict = defaultdict(lambda: None, {node:concept for node, _, concept in g.instances()})\n",
    "            reentrancies = defaultdict(lambda: None, g.reentrancies())\n",
    "            \n",
    "            changed_instances = {}\n",
    "            nodes_to_delete = []\n",
    "            epidata = {}\n",
    "            \n",
    "            for triple in g.triples:\n",
    "                if triple[0] not in nodes_to_delete and triple[2] not in nodes_to_delete:\n",
    "                    if triple[1] == rel:\n",
    "                        invoked = triple[0]\n",
    "                        nodes_below_invoked = nodes_below_dict[invoked]\n",
    "                        nodes_below_invoked_with_invoked = nodes_below_invoked + [invoked]\n",
    "                        instances_below_invoked = [instances_dict[node] for node in nodes_below_invoked]\n",
    "                        \n",
    "                        span = [alignments_dict[node] for node in nodes_below_invoked_with_invoked if alignments_dict[node]]\n",
    "                        subtree_token_span = AMRAnalysis.full_span(span)\n",
    "                        reentrancies_below_invoked = any([reentrancies[node] for node in nodes_below_invoked])\n",
    "                        \n",
    "                        if subtree_token_span and not reentrancies_below_invoked:\n",
    "                            merged = [toks[i] for i in subtree_token_span]\n",
    "                            changed_instances[invoked] = '_'.join(merged)\n",
    "                            nodes_to_delete += nodes_below_invoked\n",
    "                            continue\n",
    "                            \n",
    "                    epidata[triple] = g.epidata[triple]\n",
    "                    triples_filtered.append(triple)\n",
    "            \n",
    "            for i in range(len(triples_filtered)):\n",
    "                n, r, c = triples_filtered[i]\n",
    "                old_tuple = (n, r, c)\n",
    "                if n in changed_instances and r == ':instance':\n",
    "                    new_tuple = (n, r, changed_instances[n])\n",
    "                    triples_filtered[i] = new_tuple\n",
    "                    epidata = {(k if k != old_tuple else new_tuple):(v if k != old_tuple else v+[penman.layout.Pop()]) \n",
    "                               for k, v in epidata.items()}\n",
    "            \n",
    "            new_g = Graph(triples=triples_filtered, epidata=epidata)            \n",
    "            self.graphs_concat_rel[amr_id] = (g, new_g)\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_path_a = Path('.')/'AMR2Text'/'processed'/'corpus_a.mrp.new_aligned.txt'\n",
    "amr_path_b = Path('.')/'AMR2Text'/'processed'/'corpus_b.mrp.new_aligned.txt'\n",
    "\n",
    "amr_analysis_a = AMRAnalysis(amr_path_a, keep_meta=True, concat_rel=True)\n",
    "amr_analysis_b = AMRAnalysis(amr_path_b, keep_meta=True, concat_rel=True)\n",
    "\n",
    "graphs_concat_rel_a = amr_analysis_a.graphs_concat_rel \n",
    "graphs_concat_rel_b = amr_analysis_b.graphs_concat_rel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chinese', 'lunar', 'rover', 'lands', 'on', 'moon']\n",
      "(MRPNode-0 / land-01\n",
      "           :ARG1 (MRPNode-1 / rover\n",
      "                            :mod (MRPNode-2 / country)\n",
      "                            :mod (MRPNode-3 / moon)\n",
      "                            :mod (MRPNode-4 / name\n",
      "                                            :op1 (MRPNode-5 / china)))\n",
      "           :location (MRPNode-6 / moon))\n",
      "\n",
      "(MRPNode-0 / land-01\n",
      "           :ARG1 (MRPNode-1 / chinese_lunar_rover)\n",
      "           :location (MRPNode-6 / moon))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_dicts = [amr_analysis_a.info_dict, amr_analysis_b.info_dict]\n",
    "\n",
    "print(info_dicts[0]['4']['toks'])\n",
    "pprint(info_dicts[0]['4']['amr_string'])\n",
    "pprint(graphs_concat_rel_a['4'][1])\n",
    "#pprint(info_dicts[0]['4']['alignments_dict'])\n",
    "#pprint(info_dicts[0]['4']['labels_dict'])\n",
    "#pprint(AMRAnalysis.find_below(info_dicts[0]['22']['labels_dict']))\n",
    "#pprint(info_dicts[0]['4']['amr_graph'].epidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_a = Path('.')/'amr_suite'/'py3-Smatch-and-S2match'/'amr_data'/'corpus_a_concat.amr'\n",
    "path_b = Path('.')/'amr_suite'/'py3-Smatch-and-S2match'/'amr_data'/'corpus_b_concat.amr'\n",
    "\n",
    "def save_concatenation_results(path, amr_analysis):\n",
    "    with open(path, 'w') as f:\n",
    "        for amr_id, (_, g_concat) in amr_analysis.graphs_concat_rel.items():\n",
    "            meta_block = amr_analysis.info_dict[amr_id]['meta']\n",
    "            print(meta_block, file=f)\n",
    "            pprint(g_concat, file=f)\n",
    "            \n",
    "save_concatenation_results(path_a, amr_analysis_a)\n",
    "save_concatenation_results(path_b, amr_analysis_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic version of the concatenation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(MRPNode-0 / possible-01\n",
      "           :ARG1 (MRPNode-1 / wrong-02\n",
      "                            :ARG1 (MRPNode-2 / amr-unknown)\n",
      "                            :ARG2 (MRPNode-3 / air_i_conditioner))) \n",
      "\n",
      "(MRPNode-0 / possible-01\n",
      "           :ARG1 (MRPNode-1 / wrong-02\n",
      "                            :ARG1 (MRPNode-2 / amr-unknown)\n",
      "                            :ARG2 (MRPNode-3 / conditioner\n",
      "                                             :mod (MRPNode-4 / i)\n",
      "                                             :mod (MRPNode-5 / air))))\n"
     ]
    }
   ],
   "source": [
    "def concat_rel(g, rel=':mod'):    \n",
    "    forbidden_nodes_with_instances = {}\n",
    "    triples_filtered = []\n",
    "    for triple in g.triples:\n",
    "        if triple[1] == rel:\n",
    "            invoked = triple[0]\n",
    "            forbidden_node = triple[2]\n",
    "            instance = [concept for node, _, concept in g.instances() if node == forbidden_node]\n",
    "            if instance:\n",
    "                forbidden_nodes_with_instances[forbidden_node] = (instance[0], invoked)\n",
    "            else:\n",
    "                forbidden_nodes_with_instances[forbidden_node] = ('', invoked)\n",
    "        else:\n",
    "            triples_filtered.append(triple)       \n",
    "    for forbidden_node in forbidden_nodes_with_instances:\n",
    "        instance, invoked = forbidden_nodes_with_instances[forbidden_node]\n",
    "        for i in range(len(triples_filtered)):\n",
    "            n, r, c = triples_filtered[i]\n",
    "            if n == invoked and r == ':instance' and c != 'amr-unknown':\n",
    "                triples_filtered[i] = (n, r, f'{instance}_{c}')\n",
    "    triples_filtered = [t for t in triples_filtered if t[0] not in forbidden_nodes_with_instances]\n",
    "    epidata = {(n, r, c):g.epidata[(n, r, c.split('_')[-1])] for n, r, c in triples_filtered}\n",
    "    new_g = Graph(triples=triples_filtered, epidata=epidata)    \n",
    "    return new_g\n",
    "\n",
    "new_g = concat_rel(g, ':mod')\n",
    "print(penman.encode(new_g), '\\n')\n",
    "print(penman.encode(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SBert models vs GloVe 6B.100d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(a,b):\n",
    "    #cosine similarity\n",
    "    dist = cosine(a,b)\n",
    "    sim = 1 - min(1,dist)\n",
    "    return sim\n",
    "\n",
    "def load_glove(fp):\n",
    "    dic={}\n",
    "    if not fp:\n",
    "        return dic\n",
    "    with open(fp,\"r\") as f:\n",
    "        for line in f:\n",
    "            ls = line.split()\n",
    "            word = ls[0]\n",
    "            vec = np.array([float(x) for x in ls[1:]])\n",
    "            dic[word] = vec\n",
    "    return dic\n",
    "\n",
    "def vecs_of_sents(m, sents):\n",
    "    s_vs = np.asarray([np.sum([m[word] for word in sent.split()], axis=0)/len(sent.split()) for sent in sents])\n",
    "    return s_vs\n",
    "\n",
    "def print_scores(s1, s2, cosine_scores):\n",
    "    max_s1 = max([len(s) for s in s1])\n",
    "    max_s2 = max([len(s) for s in s2])\n",
    "    \n",
    "    for i in range(cosine_scores.shape[0]):\n",
    "        for j in range(cosine_scores.shape[1]):\n",
    "            print(f'{s1[i]:{max_s1}}\\t{s2[j]:{max_s2}}\\tScore: {cosine_scores[i, j]:.4f}')\n",
    "            \n",
    "def sbert_sim(model, s1, s2):\n",
    "    embeddings1 = model.encode(s1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(s2, convert_to_tensor=True)\n",
    "    \n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    \n",
    "    print_scores(s1, s2, cosine_scores)\n",
    "            \n",
    "def glove_sim(model, s1, s2):\n",
    "    embeddings1 = vecs_of_sents(model, s1)\n",
    "    embeddings2 = vecs_of_sents(model, s2)\n",
    "    \n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    \n",
    "    print_scores(s1, s2, cosine_scores)\n",
    "\n",
    "\n",
    "sbert1 = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "sbert2 = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "glove = load_glove('amr_suite/vectors/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"paraphrase-distilroberta-base-v1\":\n",
      "How do I pump up water pressure in my shower?\tHow can I boost the water pressure in my shower?\tScore: 0.9333\n",
      "\n",
      "\n",
      "\"distilbert-base-nli-stsb-mean-tokens\":\n",
      "How do I pump up water pressure in my shower?\tHow can I boost the water pressure in my shower?\tScore: 0.9025\n",
      "\n",
      "\n",
      "\"GloVe average\":\n",
      "how do i pump up water pressure in my shower ?\thow can i boost the water pressure in my shower ?\tScore: 0.9868\n"
     ]
    }
   ],
   "source": [
    "#s1 = ['french fries']\n",
    "#s2 = ['chip', 'chips']\n",
    "s1 = ['How do I pump up water pressure in my shower?']\n",
    "s2 = ['How can I boost the water pressure in my shower?']\n",
    "\n",
    "s1_glove = ['how do i pump up water pressure in my shower ?']\n",
    "s2_glove = ['how can i boost the water pressure in my shower ?']\n",
    "\n",
    "print('\"paraphrase-distilroberta-base-v1\":')\n",
    "sbert_sim(sbert2, s1, s2)\n",
    "print('\\n')\n",
    "print('\"distilbert-base-nli-stsb-mean-tokens\":')\n",
    "sbert_sim(sbert1, s1, s2)\n",
    "print('\\n')\n",
    "print('\"GloVe average\":')\n",
    "glove_sim(glove, s1_glove, s2_glove)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
