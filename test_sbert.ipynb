{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "m = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2129, 2079, 1045, 10216, 2039, 2300, 3778, 1999, 2026, 6457, 1029] 11\n",
      "[2129, 2064, 1045, 12992, 1996, 2300, 3778, 1999, 2026, 6457, 1029] 11\n",
      "tensor([[  101,  2129,  2079,  1045, 10216,  2039,  2300,  3778,  1999,  2026,\n",
      "          6457,  1029,   102,     0,     0]])\n",
      "tensor([[  101,  2129,  2064,  1045, 12992,  1996,  2300,  3778,  1999,  2026,\n",
      "          6457,  1029,   102,     0,     0]])\n",
      "[10216, 2039] [12992]\n",
      "[3, 4] [3]\n",
      "0.813988208770752\n"
     ]
    }
   ],
   "source": [
    "def x_in_y(query, base):\n",
    "    try:\n",
    "        l = len(query)\n",
    "    except TypeError:\n",
    "        l = 1\n",
    "        query = type(base)((query,))\n",
    "\n",
    "    for i in range(len(base)):\n",
    "        if base[i:i+l] == query:\n",
    "            return list(range(i, i+l))\n",
    "    return False\n",
    "\n",
    "def sbert_sim(sents, phrase1, phrase2):\n",
    "\n",
    "    if not (phrase1 and phrase2):\n",
    "        return 0.0\n",
    "    \n",
    "    sents_ids = m.tokenize(sents)\n",
    "    sents_embs = m.encode(sents_ids, convert_to_tensor=True,\n",
    "                          is_pretokenized=True,\n",
    "                          output_value='token_embeddings')\n",
    "    \n",
    "    tokenized1 = m.tokenize(phrase1)\n",
    "    tokenized2 = m.tokenize(phrase2)\n",
    "    \n",
    "    print(sents_ids[0], len(sents_ids[0]))\n",
    "    print(sents_ids[1], len(sents_ids[1])) \n",
    "    print(m.get_sentence_features(sents_ids[0], 12)['input_ids'])\n",
    "    print(m.get_sentence_features(sents_ids[1], 12)['input_ids'])\n",
    "    #print(sents_embs.shape)\n",
    "    \n",
    "    print(tokenized1, tokenized2)\n",
    "    phrase1_embs_indices = x_in_y(tokenized1, sents_ids[0])\n",
    "    phrase2_embs_indices = x_in_y(tokenized2, sents_ids[1])\n",
    "    print(phrase1_embs_indices, phrase2_embs_indices)\n",
    "    \n",
    "    if not (phrase1_embs_indices and phrase2_embs_indices):\n",
    "        return 0.0\n",
    "    \n",
    "    sents_embs_new = sents_embs[:, 1:].detach().clone() # Because embeddings include 'CLS'-token, but .tokenize() does not!\n",
    "    \n",
    "    return torch.nn.CosineSimilarity(0)(torch.mean(sents_embs_new[0, phrase1_embs_indices], 0), \n",
    "                                        torch.mean(sents_embs_new[1, phrase2_embs_indices], 0)).item()\n",
    "\n",
    "#How do I pump up water pressure in my shower?\tHow can I boost the water pressure in my shower?\n",
    "\n",
    "sents = ['How do I pump up water pressure in my shower?', 'How can I boost the water pressure in my shower?']\n",
    "\n",
    "phrase1 = 'pump up'\n",
    "phrase2 = 'boost'\n",
    "\n",
    "print(sbert_sim(sents, phrase1, phrase2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1996, 2126, 2008, 1037, 1000, 2744, 1000, 2003, 4225, 9041, 2006, 1996, 4646, 1012], [2057, 4521, 2413, 22201, 1012]] [1996, 4646]\n",
      "14\n",
      "[11, 12]\n",
      "torch.Size([2, 17, 768])\n"
     ]
    }
   ],
   "source": [
    "print(ids, ids_query)\n",
    "print(len(ids[0]))\n",
    "print(x_in_y(ids_query, ids[0]))\n",
    "print(embeddings1.shape)\n",
    "#print(m.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['apple', 'transformer']\n",
    "apple, transformer = m.encode(a, convert_to_tensor=True)\n",
    "apple.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1996, 4646]\n",
      "1996 -> the\n",
      "2126 -> way\n",
      "2008 -> that\n",
      "1037 -> a\n",
      "1000 -> \"\n",
      "2744 -> term\n",
      "1000 -> \"\n",
      "2003 -> is\n",
      "4225 -> defined\n",
      "9041 -> depends\n",
      "2006 -> on\n",
      "1996 -> the\n",
      "4646 -> application\n",
      "1012 -> .\n",
      "[PAD]\n",
      "{'input_ids': tensor([[ 101, 1996, 2126, 2008, 1037, 1000, 2744, 1000, 2003, 4225, 9041, 2006,\n",
      "         1996, 4646, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "ids = m.tokenize([sents[0]])[0]\n",
    "for i in ids:\n",
    "    print(i, '->', m.tokenizer.decode([i])) \n",
    "    \n",
    "print(m.tokenizer.decode([0]))\n",
    "print(m.get_sentence_features(m.tokenize([sents[0]])[0], m.max_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr_file1 = 'analysis/sts/s2match_sbert/STS2016_corpus_a_reif.amr'\n",
    "amr_file2 = 'analysis/sts/s2match_sbert/STS2016_corpus_b_reif.amr'\n",
    "\n",
    "sbert_model = m\n",
    "\n",
    "with open(amr_file1) as f1, open(amr_file2) as f2:\n",
    "    amrs1 = f1.read().strip().split('\\n\\n')\n",
    "    amrs2 = f2.read().strip().split('\\n\\n')\n",
    "\n",
    "amr_len = min(len(amrs1), len(amrs2))\n",
    "#amr_len = 16\n",
    "amrs_sents1 = [amr.split('\\n')[1].strip()[len('# ::snt '):] for amr in amrs1[:amr_len]]\n",
    "amrs_sents2 = [amr.split('\\n')[1].strip()[len('# ::snt '):] for amr in amrs2[:amr_len]]    \n",
    "\n",
    "amrs_sents = [None]*2*amr_len\n",
    "amrs_sents[::2] = amrs_sents1\n",
    "amrs_sents[1::2] = amrs_sents2\n",
    "\n",
    "sents_encoded = sbert_model.tokenize(amrs_sents)\n",
    "\n",
    "sents_embs = sbert_model.encode(amrs_sents, convert_to_tensor=False,\n",
    "                                convert_to_numpy=False,\n",
    "                                output_value='token_embeddings',\n",
    "                                is_pretokenized=False,\n",
    "                                show_progress_bar=False)\n",
    "#print(sents_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "for sent in sents_embs:\n",
    "    s.append(sent.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1866, 39, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.nn.utils.rnn.pad_sequence(sents_embs, batch_first=True)\n",
    "s.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
