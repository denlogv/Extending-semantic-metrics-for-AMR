{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Test scripts for Benepar, SuPar, SentenceTransformers APIs</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from supar import Parser\n",
    "from nltk.tree import Tree\n",
    "import spacy; nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constituency Parsing using Benepar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en2 to C:\\Users\\Denis\n",
      "[nltk_data]     Logvinenko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en2 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import benepar\n",
    "from benepar.spacy_plugin import BeneparComponent\n",
    "\n",
    "benepar_ver = 'benepar_en2'\n",
    "benepar.download(benepar_ver)\n",
    "nlp.add_pipe(BeneparComponent(benepar_ver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (PRP We))\n",
      "  (VP\n",
      "    (VBP eat)\n",
      "    (NP (DT the) (JJ all) (JJ -) (JJ known) (JJ french) (NNS fries))\n",
      "    (PP (IN from) (NP (DT the) (NN kiosk)))))\n",
      "         S                                               \n",
      "  _______|____________                                    \n",
      " |                    VP                                 \n",
      " |    ________________|______________________             \n",
      " |   |           |                           PP          \n",
      " |   |           |                       ____|___         \n",
      " NP  |           NP                     |        NP      \n",
      " |   |    _______|_________________     |     ___|____    \n",
      "PRP VBP  DT  JJ  JJ   JJ    JJ    NNS   IN   DT       NN \n",
      " |   |   |   |   |    |     |      |    |    |        |   \n",
      " We eat the all  -  known french fries from the     kiosk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\"The kitten is running through a gate\", \"A young cat sprints\"]\n",
    "texts  = ['We eat the all-known french fries from the kiosk']\n",
    "docs = list(nlp.pipe(texts))\n",
    "trees = [Tree.fromstring(list(doc.sents)[0]._.parse_string) for doc in docs]\n",
    "\n",
    "tree = trees[0]\n",
    "print(tree)\n",
    "tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP (PRP We))\n",
      " NP\n",
      " |  \n",
      "PRP\n",
      " |  \n",
      " We\n",
      "\n",
      "\n",
      "(NP (DT the) (JJ all) (JJ -) (JJ known) (JJ french) (NNS fries))\n",
      "         NP                   \n",
      "  _______|_________________    \n",
      " DT  JJ  JJ   JJ    JJ    NNS \n",
      " |   |   |    |     |      |   \n",
      "the all  -  known french fries\n",
      "\n",
      "\n",
      "(NP (DT the) (NN kiosk))\n",
      "     NP      \n",
      "  ___|____    \n",
      " DT       NN \n",
      " |        |   \n",
      "the     kiosk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<class 'nltk.tree.Tree'>\n",
      "['We']\n",
      "<class 'nltk.tree.Tree'>\n",
      "['the', 'all', '-', 'known', 'french', 'fries']\n",
      "<class 'nltk.tree.Tree'>\n",
      "['the', 'kiosk']\n"
     ]
    }
   ],
   "source": [
    "nps = list(tree.subtrees(filter=lambda x: x.label()=='NP'))\n",
    "for np in nps: print(np); np.pretty_print(); print()\n",
    "\n",
    "print('\\n')\n",
    "for np in nps:\n",
    "    print(np.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[We, eat the all-known french fries from the kiosk]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = list(docs[0].sents)[0]\n",
    "list(sent._.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependecy Parsing using SuPar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser.load('crfnp-dep-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|####################################| 1/1 00:00<00:00, 32.38it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\tThe\t_\t_\t_\t_\t2\tdet\t_\t_\n",
      "2\thouse\t_\t_\t_\t_\t2\tamod\t_\t_\n",
      "3\twas\t_\t_\t_\t_\t4\tauxpass\t_\t_\n",
      "4\tsold\t_\t_\t_\t_\t4\tccomp\t_\t_\n",
      "5\tin\t_\t_\t_\t_\t4\tprep\t_\t_\n",
      "6\ttime\t_\t_\t_\t_\t5\tpobj\t_\t_\n",
      "\n",
      "1\tThey\t_\t_\t_\t_\t2\tnsubj\t_\t_\n",
      "2\tsold\t_\t_\t_\t_\t2\tdep\t_\t_\n",
      "3\tthe\t_\t_\t_\t_\t4\tdet\t_\t_\n",
      "4\thouse\t_\t_\t_\t_\t4\tdep\t_\t_\n",
      "5\tin\t_\t_\t_\t_\t2\tprep\t_\t_\n",
      "6\ttime\t_\t_\t_\t_\t5\tpobj\t_\t_\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = ['The house was sold in time', 'They sold the house in time']\n",
    "docs = list(nlp.pipe(text))\n",
    "toks = [[tok.text for tok in doc] for doc in docs]; print(len(toks))\n",
    "dataset = parser.predict(toks, prob=True, verbose=False)\n",
    "pars = dataset.sentences\n",
    "\n",
    "for par in pars: print(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Phrase Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kitten \t\t A young cat \t\t Score: 0.6921\n",
      "The kitten \t\t a mouse \t\t Score: 0.2679\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "s1 = ['Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.']\n",
    "s2 = ['Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.']\n",
    "\n",
    "embeddings1 = model.encode(s1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(s2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(cosine_scores.shape[0]):\n",
    "    for j in range(cosine_scores.shape[1]):\n",
    "        print(f'{s1[i]} \\t\\t {s2[j]} \\t\\t Score: {cosine_scores[i, j]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
